<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Intro to Regression: Part 3: Covariance and correlation</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>Intro to Regression: Part 3: Covariance and correlation</h2>

<p>At the heart of regression analysis is the measure of <strong><em>covariance</em></strong> and
<strong><em>correlation</em></strong> between variables.  Covariance measures how two variables vary with
respect to each other (how much they &ldquo;move together&rdquo;).</p>

<p>The covariance of two variables, X and Y, is given by the below equation:</p>

<p>\[ Cov(X,Y) = \frac{1}{N-1} \cdot \sum_i^N (X_i - \overline{X})(Y_i - \overline{Y}) \]</p>

<p>Technically speaking, this is the <strong><em>sample covariance</em></strong>.  Note the degrees of
freedom, N-1, where the -1 accounts for the estimation of the variables&#39; true
means using their respective sample means.</p>

<p>Compare the equation for sample covariance above with the equation for sample variance:</p>

<p>\[
\begin{align*}
Var(X) &= \frac{1}{N-1} \cdot \sum_i^N (X_i - \overline{X})^2
\\[8pt]
&= \frac{1}{N-1} \cdot \sum_i^N (X_i - \overline{X})(X_i - \overline{X})
\end{align*}
\]</p>

<p>One could say that a variable&#39;s sample variance is the same as its covariance
with itself.  (I&#39;m not sure if this provides any sort of useful insight).</p>

<p>The correlation between two variables is just the &ldquo;normalized&rdquo; covariance.  By
&ldquo;normalized&rdquo; I mean the variables are converted to z-scores.  A z-score is
calculated using the below equation:</p>

<p>\[
Z = \frac{X - \overline{X}}{\sigma_x}
\]</p>

<p>We&#39;ve already subtracted the means in the sample covariance formula, so to convert it to the correlation we need to divide by the standard devations of both X and Y:</p>

<p>\[
Cor(X,Y) = \frac{ Cov(X,Y) } {\sigma_x \sigma_y}
\]</p>

<p>The correlation between two variables always falls in the range between +1 and -1.  </p>

<ul>
<li>+1 = a perfectly linear relationship<br/>
A perfectly linear relationship means that a 1-unit change in X corresponds to a 1-unit change in Y. </li>
<li>-1 = a perfectly inverse linear relationship<br/>
A perfectly inverse linear relationship  means that a 1-unit change in X corresponds to a negative 1-unit change in Y.</li>
<li>0 = no relationship at all<br/>
No relationship at all means that X and Y don&#39;t move together.  A 1-unit change in X tells you nothing about the change in Y.</li>
</ul>

<p><br /></p>

<h3>How is the correlation applied to the linear model?</h3>

<p>A linear model relating Y (the response) to X (the predictor) takes on the form:</p>

<p>\[
Y = \beta_0 + \beta_1 X + \epsilon
\]</p>

<p>The coefficient \(\beta_1\), which relates X to Y, is determined by the
correlation between the two variables.  In this simple model (with a single
predictor variable), \(\beta_1\) is calculated by the below equation:</p>

<p>\[
\beta_1 = Cor(X,Y) \cdot \frac{\sigma_y}{\sigma_x}
\]</p>

<p><em>Aside: the calculation of \(\beta_1\) is a bit more complex when you add more predictor 
variables to the model.  We&#39;ll cover that later.</em></p>

<p>Let&#39;s step back and make sense of this.  Cor(X,Y) is the &ldquo;normalized&rdquo;
covariance, which means it&#39;s the ratio of &ldquo;normalized&rdquo; movements in X to
&ldquo;normalized&rdquo; movements in Y.  Multiplying by the standard deviation of Y
effectively &ldquo;de-normalizes&rdquo; it, to get us back to the actual (non-normalized)
values of Y.  </p>

<p>So how come we divide by the standard deviation of X?  Well, because we&#39;re
multiplying the whole thing by the (non-normalized) values of X.  So
technically we have to first &ldquo;normalize&rdquo; the X value (by dividing it by its
standard deviation), then apply the correlation (the &ldquo;normalized&rdquo;
covariance), which gives is the &ldquo;normalized&rdquo; movement in Y, which
we then multiply by the standard deviation of Y to get back to the
(non-normalized) values of Y.</p>

<p>Maybe it&#39;s a little clearer if we re-arrange the terms:</p>

<p>\[
\begin{align}
Y &= \beta_0 + \beta_1 \cdot X  \\[8pt]
&= \beta_0  + Cor(X,Y) \cdot \frac{\sigma_y}{\sigma_x} \cdot X  \\[8pt]
&= \beta_0 + Cor(X,Y) \cdot \frac{X}{\sigma_x} \cdot \sigma_y  
\end{align} 
\]</p>

<p><br /></p>

<h3>The units of covariance, correlation, and coefficients</h3>

<p>It sometimes helps me understand things by looking at the units.</p>

<ul>
<li>The units of covariance are (units of X)*(units of Y), since X and Y are multiplied together in the covariance equation. </li>
<li>Correlation is unit-less, since the units of covariance (X*Y) are cancelled
out when dividing by the standard deviations of both X and Y, which
respectively have (units of X) and (units of Y)</li>
<li>The units of \(\beta_1\), the coefficient of X, are (units of Y) / (units of X).  When
\(\beta_1\) is multipled by X, the (units of X) cancel out, leaving just (units of Y).</li>
</ul>

</body>

</html>
